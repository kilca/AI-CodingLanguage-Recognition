{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kilian Cannet - Noe Boonstra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sample_code.csv\", lineterminator='\\n',names=['content','lang'])\n",
    "\n",
    "#Reduire taille trop grande\n",
    "#Todo voir si utile\n",
    "len_cond = [True if 20<=len(s)<=10000 else False for s in data['content']]\n",
    "data = data[len_cond]\n",
    "\n",
    "#Filtrer\n",
    "lang = ['csv', 'txt', 'java', 'cs', 'py', 'cpp','c']\n",
    "data = data[data['lang'].isin(lang)]\n",
    "\n",
    "data_trim= pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "'''\n",
    "for l in lang:\n",
    "    lang_trim = data[data['lang'] ==l].sample(50000,random_state = 100)\n",
    "    data_trim = data_trim.append(lang_trim)\n",
    "'''\n",
    "#Melager\n",
    "data_shuffle = data_trim.sample(frac=1)\n",
    "\n",
    "maxim = data_shuffle.shape[0]\n",
    "percent = 0.8\n",
    "part = int(maxim*percent)\n",
    "train = data_shuffle[0:part]\n",
    "valid = data_shuffle[part:maxim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8920496424231559\n",
      "0.8678884462151395\n",
      "['java']\n",
      "['cpp']\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB())])\n",
    "clf = clf.fit(train[\"content\"],train[\"lang\"])\n",
    "print(clf.score(train[\"content\"],train[\"lang\"]))\n",
    "print(clf.score(valid[\"content\"],valid[\"lang\"]))\n",
    "\n",
    "print(clf.predict([\"public class HelloWorld {\\\n",
    "\\\n",
    "    public static void main(String[] args) {\\\n",
    "        // fait qqc\\\n",
    "        System.out.println(2);\\\n",
    "    }\\\n",
    "\\\n",
    "}\"]))\n",
    "\n",
    "print(clf.predict([\"\\\n",
    "\\\n",
    "    void main() {\\\n",
    "        // fait qqc\\\n",
    "        std::cout << \\\"Hello`\\\";\\\n",
    "    }\\\n",
    "\\\n",
    "}\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bordel a revoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_trigrams(corpus,n_feat=200):\n",
    "    \"\"\"\n",
    "    Returns a list of the N most common character trigrams from a list of sentences\n",
    "    params\n",
    "    ------------\n",
    "        corpus: list of strings\n",
    "        n_feat: integer\n",
    "    \"\"\"\n",
    "    \n",
    "    #fit the n-gram model\n",
    "    vectorizer = CountVectorizer(analyzer='char',\n",
    "                            ngram_range=(3, 3)\n",
    "                            ,max_features=n_feat)\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    #Get model feature names\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain trigrams from each language\n",
    "features = {}\n",
    "features_set = set()\n",
    "\n",
    "for l in lang:\n",
    "    \n",
    "    #get corpus filtered by language\n",
    "    corpus = train[train.lang==l]['content']\n",
    "    \n",
    "    #get 200 most frequent trigrams\n",
    "    trigrams = get_trigrams(corpus)\n",
    "    \n",
    "    #add to dict and set\n",
    "    features[l] = trigrams \n",
    "    features_set.update(trigrams)\n",
    "\n",
    "    \n",
    "#create vocabulary list using feature set\n",
    "vocab = dict()\n",
    "for i,f in enumerate(features_set):\n",
    "    vocab[f]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train count vectoriser using vocabulary\n",
    "vectorizer = CountVectorizer(analyzer='char',\n",
    "                             ngram_range=(3, 3),\n",
    "                            vocabulary=vocab)\n",
    "\n",
    "#create feature matrix for training set\n",
    "corpus = train['content']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "train_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale feature matrix \n",
    "train_min = train_feat.min()\n",
    "train_max = train_feat.max()\n",
    "train_feat = (train_feat - train_min)/(train_max-train_min)\n",
    "\n",
    "#Add target variable \n",
    "train_feat['lang'] = list(train['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create feature matrix for validation set\n",
    "corpus = valid['content']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "valid_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
    "valid_feat = (valid_feat - train_min)/(train_max-train_min)\n",
    "valid_feat['lang'] = list(valid['lang'])\n",
    "\n",
    "#create feature matrix for test set\n",
    "corpus = test['content']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "test_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
    "test_feat = (test_feat - train_min)/(train_max-train_min)\n",
    "test_feat['lang'] = list(test['lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-090fe87f5ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Fit encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#Fit encoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(['deu', 'eng', 'fra', 'ita', 'por', 'spa'])\n",
    "\n",
    "def encode(y):\n",
    "    \"\"\"\n",
    "    Returns a list of one hot encodings\n",
    "    Params\n",
    "    ---------\n",
    "        y: list of language labels\n",
    "    \"\"\"\n",
    "    \n",
    "    y_encoded = encoder.transform(y)\n",
    "    y_dummy = np_utils.to_categorical(y_encoded)\n",
    "    \n",
    "    return y_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Get training data\n",
    "x = train_feat.drop('lang',axis=1)\n",
    "y = encode(train_feat['lang'])\n",
    "\n",
    "#Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=663, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Train model\n",
    "model.fit(x, y, epochs=4, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-f9417549f0ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Get predictions on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "x_test = test_feat.drop('lang',axis=1)\n",
    "y_test = test_feat['lang']\n",
    "\n",
    "#Get predictions on test set\n",
    "labels = model.predict_classes(x_test)\n",
    "predictions = encoder.inverse_transform(labels)\n",
    "\n",
    "#Accuracy on test set\n",
    "accuracy = accuracy_score(y_test,predictions)\n",
    "print(accuracy)\n",
    "\n",
    "#Create confusion matrix\n",
    "lang = ['csv', 'txt', 'java', 'cs', 'py', 'cpp']\n",
    "conf_matrix = confusion_matrix(y_test,predictions)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix,columns=lang,index=lang)\n",
    "\n",
    "#Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False)\n",
    "plt.xlabel('Predicted',fontsize=22)\n",
    "plt.ylabel('Actual',fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115538\n"
     ]
    }
   ],
   "source": [
    "#-------------------------\n",
    "print(train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cpp']\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([\"\\\n",
    "\\\n",
    "    void main() {\\\n",
    "        // fait qqc\\\n",
    "        std::cout << \\\"Hello`\\\";\\\n",
    "    }\\\n",
    "\\\n",
    "}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
